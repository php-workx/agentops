# AgentOps

> **"Every AI coding tool forgets. AgentOps remembers."**

AI-assisted development workflows that compound knowledge over time.

---

## Why AgentOps?

### The Problem: AI Amnesia

Your AI assistant is brilliant in the moment and amnesiac across sessions.

You debug an OAuth token refresh issue. 45 minutes, $2.40 in tokens. Done.

Three weeks later, same issue. Claude starts fresh. Another 45 minutes. Another $2.40.

**This isn't a bug. It's thermodynamics.**

### The Science: Knowledge Decays

Research (Darr, Argote, & Epple) measured organizational knowledge decay: **17% per week**.

```
Week 0: 100% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Week 1:  83% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘
Week 2:  69% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘
Week 4:  47% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
```

Every AI session starts at Week 0. By the time you need that knowledge again, it's gone.

### The Math: Escape Velocity

Knowledge evolution follows a differential equation:

```
dK/dt = I(t) - Î´Â·K + ÏƒÂ·ÏÂ·K

Where:
  I(t) = new knowledge input
  Î´    = decay rate (0.17/week)
  Ïƒ    = retrieval effectiveness (can you find it?)
  Ï    = citation rate (do you use it?)
```

**The critical insight:** `ÏƒÂ·ÏÂ·K` is the compounding term. When retrieval Ã— usage exceeds decay, knowledge grows exponentially.

```
Escape velocity: Ïƒ Ã— Ï > Î´

Without AgentOps:  0 Ã— 0 = 0.00 < 0.17  â†’ Always decaying
With AgentOps:    0.7 Ã— 0.3 = 0.21 > 0.17  â†’ Compounding
```

That 0.04 difference compounds. After a year, it's the difference between an assistant that knows nothing and one that knows your entire codebase.

### The Result: Compounding Intelligence

```
WITHOUT AGENTOPS:
  Session 1: Debug OAuth refresh (45 min, $2.40)
  Session 2: Same issue, fresh start (45 min, $2.40)
  Session 3: Same issue, fresh start (45 min, $2.40)
  Total: 135 min, $7.20 â€” and still forgetting

WITH AGENTOPS:
  Session 1: Debug OAuth, capture pattern (45 min, $2.40)
  Session 2: "I see we solved this before" (3 min, $0.15)
  Session 3: Instant recall (1 min, $0.05)
  Total: 49 min, $2.60 â€” and getting faster
```

**64% time savings. 64% cost savings. And it compounds.**

---

## How It Works

### The Knowledge Flywheel

```
    CAPTURE â”€â”€â”€â”€â”€â”€â–º STORE â”€â”€â”€â”€â”€â”€â–º RECALL
        â”‚             â”‚             â”‚
        â”‚             â”‚             â–¼
        â”‚             â”‚          APPLY
        â”‚             â”‚             â”‚
        â”‚             â”‚             â–¼
        â”‚             â”‚          LEARN
        â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              (compounds forever)
```

1. **Capture** â€” `/forge` extracts decisions and learnings from sessions
2. **Store** â€” Dual format (human-readable + machine-queryable) in `.agents/`
3. **Recall** â€” `/inject` retrieves relevant knowledge at session start
4. **Apply** â€” Knowledge used in your work
5. **Learn** â€” Utility scoring tracks what's actually helpful

Each turn makes the next turn easier. That's the flywheel.

### The Brownian Ratchet

Progress is permanent. You can always add more chaos, but you can't un-ratchet.

```
CHAOS â”€â”€â”€â”€â–º FILTER â”€â”€â”€â”€â–º RATCHET
  â”‚            â”‚            â”‚
  â”‚            â”‚            â””â”€â”€ Merged code, closed issues, stored learnings
  â”‚            â””â”€â”€ Tests, CI, /vibe, /pre-mortem
  â””â”€â”€ Multiple attempts, parallel exploration
```

From physics: random motion through a one-way gate produces net forward movement.

In practice:
- **Chaos**: Explore multiple approaches (polecats, branches, experiments)
- **Filter**: Validate ruthlessly (tests pass, security clean, quality high)
- **Ratchet**: Lock progress permanently (merge, close, store)

Once knowledge is ratcheted, it never goes backward.

---

## Install

```bash
claude mcp add boshu2/agentops
```

## Quick Start

```bash
# Research a topic (builds knowledge)
/research authentication flows

# Plan work (decomposes into issues)
/plan add OAuth refresh token handling

# Implement (executes with full context)
/implement

# Validate (quality + security + architecture)
/vibe

# Extract learnings (closes the loop)
/retro
```

---

## The RPI Workflow

```
Research â†’ Plan â†’ Implement â†’ Validate
    â†‘                            â”‚
    â””â”€â”€â”€â”€ Knowledge Flywheel â”€â”€â”€â”€â”˜
```

Every completion feeds back into research. Your assistant gets smarter about YOUR codebase.

---

## Skills

| Skill | Purpose | Trigger Phrases |
|-------|---------|-----------------|
| `/research` | Deep codebase exploration | "understand", "explore", "investigate" |
| `/plan` | Decompose goals into issues | "plan", "break down", "what issues" |
| `/implement` | Execute a single issue | "implement", "work on", "fix" |
| `/crank` | Autonomous multi-issue execution | "execute", "crank", "ship it" |
| `/vibe` | Validate code quality | "validate", "check", "review" |
| `/pre-mortem` | Simulate failures before building | "what could go wrong", "risks" |
| `/retro` | Extract learnings | "retrospective", "what did we learn" |
| `/post-mortem` | Full validation + extraction | "post-mortem", "wrap up" |
| `/forge` | Mine transcripts for knowledge | "forge", "extract knowledge" |
| `/inject` | Load relevant knowledge | "what do we know about" |
| `/beads` | Issue tracking | "create issue", "what's ready" |
| `/bug-hunt` | Root cause analysis | "investigate bug", "why is this broken" |

---

## ao CLI

The `ao` CLI provides the knowledge engine.

**Install:**
```bash
brew install agentops
# or build from source
cd cli && go build -o ao ./cmd/ao
```

**Commands:**
```bash
ao badge                    # Flywheel health (are you compounding?)
ao forge transcript <path>  # Extract knowledge from sessions
ao search <query>           # Find what you've learned
ao inject <context>         # Load knowledge for new session
ao ratchet status           # Workflow progress
ao feedback <id> <reward>   # Train utility scoring
```

**Example:**
```bash
$ ao badge
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ðŸ›ï¸  AGENTOPS KNOWLEDGE             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Sessions Mined    â”‚  47                  â•‘
â•‘  Learnings         â”‚  156                 â•‘
â•‘  Patterns          â”‚  23                  â•‘
â•‘  Citations         â”‚  89                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Retrieval (Ïƒ)     â”‚  0.72  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ â•‘
â•‘  Citation Rate (Ï) â”‚  0.34  â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ â•‘
â•‘  Decay (Î´)         â”‚  0.17  â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ÏƒÃ—Ï = 0.24 > Î´    â”‚  ðŸš€ COMPOUNDING    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Knowledge Storage

AgentOps stores knowledge in `.agents/`:

```
.agents/
â”œâ”€â”€ learnings/    # Extracted lessons (what we learned)
â”œâ”€â”€ patterns/     # Reusable solutions (how we solved it)
â”œâ”€â”€ research/     # Exploration findings (what we found)
â”œâ”€â”€ retros/       # Retrospectives (what went wrong/right)
â”œâ”€â”€ products/     # Product briefs (why we're building)
â””â”€â”€ ao/
    â”œâ”€â”€ sessions/ # Mined transcripts
    â”œâ”€â”€ index/    # Search index
    â””â”€â”€ chain.jsonl  # Ratchet state
```

**Dual format:** Every artifact has `.md` (human-readable) and `.jsonl` (machine-queryable).

---

## The Science (Deep Dive)

### Cognitive Load Theory (Sweller, 1988)

The 40% rule isn't arbitrary. Research shows performance peaks at moderate cognitive load:

```
Performance
    â”‚
100%â”‚          â•­â”€â”€â”€â•®
    â”‚        â•­â”€â•¯   â•°â”€â•®
 50%â”‚      â•­â”€â•¯       â•°â”€â”€â”€ collapse
    â”‚    â•­â”€â•¯
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0%   20%   40%   60%   80%
              Context Load
```

AgentOps checkpoints at 35%, alerts at 40%. You stay in the performance zone.

### MemRL (Zhang et al., 2026)

Not all knowledge is equally useful. MemRL uses reinforcement learning:

```python
# Utility updates based on feedback
utility = (1 - Î±) Ã— old_utility + Î± Ã— reward

# Retrieval ranks by freshness AND utility
score = z_norm(freshness) + Î» Ã— z_norm(utility)
```

The system learns what actually helps, not just what's recent.

### The Brownian Ratchet (Thermodynamics)

Random molecular motion + one-way gate = net forward movement.

In software: chaos (exploration) + filter (validation) + ratchet (merge) = permanent progress.

You can always add more chaos. You can't un-ratchet. Progress is locked.

---

## What Makes AgentOps Different

| Tool | Remembers? | Compounds? | Has Flywheel? |
|------|-----------|-----------|---------------|
| Cursor | No | No | No |
| Copilot | No | No | No |
| Devin | No | No | No |
| Claude Code | No | No | No |
| **+ AgentOps** | **Yes** | **Yes** | **Yes** |

Everyone has good AI. Nobody has the loop that makes it learn.

**The loop is the product.**

---

## Requirements

- [Claude Code](https://github.com/anthropics/claude-code) v1.0+
- Optional: [beads](https://github.com/beads-ai/beads) for issue tracking
- Optional: Go 1.22+ (to build ao CLI from source)

## Documentation

- [docs/brownian-ratchet.md](docs/brownian-ratchet.md) â€” Core philosophy
- [docs/knowledge-flywheel.md](docs/knowledge-flywheel.md) â€” How compounding works
- [docs/ao-cli.md](docs/ao-cli.md) â€” CLI reference

## License

MIT

---

> **"You sleep. Code ships. Intelligence compounds."**
